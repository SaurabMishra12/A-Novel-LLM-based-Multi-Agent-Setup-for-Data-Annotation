Fine-Tuning Configuration

Base Model: Llama-3.1‚Äì8B‚ÄìInstruct

Frameworks: Hugging Face Transformers (v4.50.0), PEFT

Fine-tuning Method: Parameter-Efficient Fine-Tuning using LoRA

Task Type: Causal Language Modeling (CAUSAL_LM)

Precision: bfloat16

Computation: Partially accelerated using GPUs on Google Cloud Platform (GCP)

LoRA Settings

Rank 
ùëü
=
16
r=16

Scaling factor 
ùõº
=
32
Œ±=32

Dropout = 0.05

Bias = none

fan_in_fan_out = false

Adapters applied to:
q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj

Model Architecture

Number of layers: 32

Hidden size: 4096

Intermediate size: 14,336

Attention heads: 32 (8 key-value heads)

Head dimension: 128

Rotary embeddings:

Max context length = 131,072

Scaling factor = 8.0

Type = llama3

Dropout:

Attention dropout = 0.0

MLP bias = false

Initialization range: 0.02

RMSNorm epsilon: 1e-5

Tokenizer

Vocabulary size: 128,257

BOS token: 128000

EOS tokens: [128001, 128008, 128009]


All LoRA and model hyperparameters are reported to support reproducibility.
Dataset and infrastructure details are intentionally withheld to prevent unauthorised replication.
